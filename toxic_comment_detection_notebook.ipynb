
# Toxic Comment Detection - Hackathon Starter Notebook üìù

## 1. Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

import warnings
warnings.filterwarnings('ignore')

## 2. Load Dataset
train = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')
test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')
submission = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')

train.head()

## 3. EDA (Exploratory Data Analysis)
print('Train shape:', train.shape)
print('Test shape:', test.shape)
train['comment_text'].apply(lambda x: len(str(x).split())).hist(bins=50)
plt.title("Comment Word Count Distribution")
plt.show()

## 4. Preprocessing with TF-IDF
tfidf = TfidfVectorizer(stop_words='english', max_features=10000)

X = tfidf.fit_transform(train['comment_text'])
y = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

## 5. Logistic Regression Model
lr = LogisticRegression()
lr.fit(X_train, y_train['toxic'])
y_pred_lr = lr.predict(X_val)
print('Logistic Regression Toxic Score:', accuracy_score(y_val['toxic'], y_pred_lr))
print(classification_report(y_val['toxic'], y_pred_lr))

## 6. Random Forest Model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train['toxic'])
y_pred_rf = rf.predict(X_val)
print('Random Forest Toxic Score:', accuracy_score(y_val['toxic'], y_pred_rf))
print(classification_report(y_val['toxic'], y_pred_rf))

## 7. Predict on Test Set (Logistic Regression Example)
X_test = tfidf.transform(test['comment_text'])
submission['toxic'] = lr.predict(X_test)
submission.to_csv('submission.csv', index=False)

## 8. Optional Bonus: BERT (HuggingFace Transformers)
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
import tensorflow as tf

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)

def encode_texts(texts, max_len=128):
    return tokenizer(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors='tf')

X_enc = encode_texts(train['comment_text'][:5000])
y_enc = tf.convert_to_tensor(y[:5000].values)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])
model.fit({'input_ids': X_enc['input_ids'], 'attention_mask': X_enc['attention_mask']}, y_enc, epochs=1, batch_size=16)

print('‚úÖ Notebook Ready - Good Luck!')
