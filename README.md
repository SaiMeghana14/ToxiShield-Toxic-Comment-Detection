# ğŸ›¡ï¸ ToxiShield: Detecting Toxic Comments Using Logistic Regression and BERT

## ğŸš€ Project Overview
ToxiShield is an AI-powered system that classifies toxic comments on online platforms. It detects six types of toxic behavior â€” **toxic**, **severe toxic**, **obscene**, **threat**, **insult**, and **identity hate** â€” using classical Machine Learning and Deep Learning models.

This project was built as part of the Meta Kaggle Hackathon under the **Main Track**, focusing on **AI for Social Good**.

---

## ğŸ“Š Dataset
- **Source:** Jigsaw Toxic Comment Classification Challenge on Kaggle
- **Training Samples:** 159,571
- **Test Samples:** 153,164
- **Labels:** Multi-label classification (six target labels)

---

## ğŸ§° Methods Used
- âœ… **EDA:** Word Count Distribution, Label Distribution, Word Clouds
- âœ… **Preprocessing:** TF-IDF Vectorization (classical ML), Tokenization (BERT)
- âœ… **Models:**
  - Logistic Regression (Baseline Model)
  - Random Forest Classifier (Ensemble Model)
  - BERT (Bonus Deep Learning Model with Huggingface Transformers)
- âœ… **Evaluation Metrics:** Accuracy, F1-Score, Classification Report

---

## ğŸ† Results Summary
| Model                | Toxic Class Accuracy  |          F1-Score        |   Notes                        |
|-----------------------|----------------------|--------------------------|--------------------------------|
| Logistic Regression   | ~95%                 |          91.3%           | Fast, efficient baseline       |
| Random Forest         | ~93%                 |          92.2%           | Robust ensemble method         |
| BERT (Bonus Model)    | High (training sample) |        94.7%           | Powerful language understanding|

---

## ğŸ“ Tech Stack
- Python, scikit-learn, matplotlib, seaborn, wordcloud
- NLP techniques: TF-IDF, Logistic Regression, Random Forest
- BERT (unitary/toxic-bert) via HuggingFace transformers
- Kaggle for execution

---

## ğŸ“ Repository Structure
```
ğŸ“ toxishield/
â”‚ â”œâ”€â”€ ğŸ“„ toxic_comment_detection_notebook.ipynb
â”‚ â”œâ”€â”€ ğŸ“„ submission.csv (optional)
â”‚ â”œâ”€â”€ ğŸ“ report/
â”‚ â”‚   â””â”€â”€ toxic_comment_report.md
â”‚ â”œâ”€â”€ ğŸ“ ppt/
â”‚ â”‚   â””â”€â”€ toxic_comment_detection_ppt.pptx
â”‚ â””â”€â”€ ğŸ“ images/
â”‚    â”œâ”€â”€ toxishield_architecture_diagram.png
â”‚    â””â”€â”€ wordcloud.png
â”‚    â””â”€â”€ screenshots/
â”‚        â””â”€â”€ eda.png
â”‚        â””â”€â”€ model_results.png
â”‚ 
ğŸ“„ README.md
ğŸ“„ LICENSE (MIT)
```
---

## ğŸš€ How to Reproduce
1. Open [Kaggle Notebook Link](https://www.kaggle.com/your-notebook-link)
2. Run all cells.

## ğŸ”— Useful Links
- [Dataset](https://www.kaggle.com/competitions/meta-kaggle-hackathon/data)
- [Kaggle Notebook](https://www.kaggle.com/your-notebook-link)
- [GitHub Repository](https://github.com/yourusername/toxishield)

---

## ğŸ“£ Credits
Made with â¤ï¸ by Sai Meghana for Meta Hackathon 2025.
